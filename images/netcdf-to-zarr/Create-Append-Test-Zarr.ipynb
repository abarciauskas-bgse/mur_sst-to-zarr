{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create, Append and Test Generating the Zarr Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numcodecs\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "import zarr\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Local Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=4)\n",
    "client = Client(cluster)\n",
    "print(f\"Dask client {client}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to generate a list of NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_list(start_doy, end_doy, year):   \n",
    "    \"\"\"\n",
    "    Given a start day and end end day, generate a list of file locations.\n",
    "    Assumes a 'prefix' and 'year' variables have already been defined.\n",
    "    'Prefix' should be a local directory or http url and path.\n",
    "    'Year' should be a 4 digit year.\n",
    "    \"\"\"\n",
    "    days_of_year = list(range(start_doy, end_doy))\n",
    "    fileObjs = []\n",
    "    for doy in days_of_year:\n",
    "        if doy < 10:\n",
    "            doy = f\"00{doy}\"\n",
    "        elif doy >= 10 and doy < 100:\n",
    "            doy = f\"0{doy}\"            \n",
    "        file = glob.glob(f\"{netcdf_prefix}/{year}/{doy}/*.nc\")[0]\n",
    "        fileObjs.append(file)\n",
    "    return fileObjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some constants\n",
    "\n",
    "* Chunking configuration\n",
    "* Location of netcdf files\n",
    "* Location of zarr store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invariants - but should be made configurable\n",
    "netcdf_prefix = f\"/s3fsx/eodc/mursst_netcdf\"\n",
    "chunks = {'time': 5, 'lat': 1799, 'lon': 3600}\n",
    "path = 'x'.join(map(str, chunks.values()))\n",
    "# CLI Argument - zarr directory\n",
    "store_dir = f\"/s3fsx/eodc/mursst_zarr/5x1799x3600\"\n",
    "numcodecs.blosc.use_threads = False\n",
    "print(f\"zarr store directory: {store_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the existing state of the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_zarr = xr.open_zarr(store_dir, consolidated=True, mask_and_scale=False)\n",
    "ds_zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop and Append to the Zarr Store\n",
    "\n",
    "* Work in batches of 5 days\n",
    "* Set start and end date\n",
    "* Once the year is complete, we will test it and write the store to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loop and append\n",
    "# Command Line Argument - year, start_day, number_batches_to_append, batch_size\n",
    "year = 2016\n",
    "start_doy = 366\n",
    "end_doy = start_doy\n",
    "number_batches_to_append = 1\n",
    "batch_size = 5\n",
    "final_end_doy = start_doy + (number_batches_to_append * batch_size)\n",
    "\n",
    "while start_doy < final_end_doy:\n",
    "    end_doy = start_doy + batch_size\n",
    "    end_doy = min(367, end_doy)\n",
    "    fileObjs = generate_file_list(start_doy, end_doy, year)\n",
    "    first_file = fileObjs[0].split('/')[-1]\n",
    "    last_file = fileObjs[-1].split('/')[-1]\n",
    "    print(f\"start doy: {start_doy}, starting file: {first_file}\")\n",
    "    print(f\"end doy: {end_doy}, ending file: {last_file}\")\n",
    "    args = {'consolidated': True}\n",
    "    # Either append or initiate store\n",
    "    if start_doy == 152 and year == 2002:\n",
    "        ds = xr.open_mfdataset(fileObjs, parallel=True, combine='by_coords', mask_and_scale=False)\n",
    "        ds = ds.chunk(chunks)       \n",
    "        args['mode'] = 'w'\n",
    "    else:\n",
    "        # Check here that the next day we will append is the next day in the year\n",
    "        current_ds = xr.open_zarr(store_dir, consolidated=True)\n",
    "        next_day = current_ds.time[-1].values + np.timedelta64(1, 'D')\n",
    "        next_day_str = str(next_day)[0:10].replace('-', '') \n",
    "        if not (first_file[0:8] == next_day_str):\n",
    "            raise Exception(\"starting file is not the next day of the year\")\n",
    "            break\n",
    "        drop_vars = ['dt_1km_data', 'sst_anomaly']\n",
    "        ds = xr.open_mfdataset(fileObjs, parallel=True, combine='by_coords', drop_variables=drop_vars)\n",
    "        ds = ds.chunk(chunks)        \n",
    "        args['mode'] = 'a'\n",
    "        args['append_dim'] = 'time'\n",
    "    ds.to_zarr(store_dir, **args)\n",
    "    start_doy = end_doy\n",
    "    print(f\"Done with this batch\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Zarr store against the Original NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slice = slice(datetime.strptime(f\"2016-12-21\", '%Y-%m-%d'), datetime.strptime(f\"2016-12-31\", '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fileObjs = generate_file_list(1, 367, 2016)\n",
    "print(f\"starting file: {fileObjs[0].split('/')[-1]}\")\n",
    "print(f\"ending file: {fileObjs[-1].split('/')[-1]}\")\n",
    "drop_vars = ['dt_1km_data', 'sst_anomaly']\n",
    "ds_netcdf = xr.open_mfdataset(fileObjs, parallel=True, combine='by_coords', mask_and_scale=False, drop_variables=drop_vars)\n",
    "ds_netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(ds_netcdf.where(ds_netcdf.analysed_sst != -32768).analysed_sst[356:366,:,:].sel(lat=slice(40,50),lon=slice(-170,-160)).mean().values)\n",
    "ds_netcdf.where(ds_netcdf.analysed_sst != -32768).analysed_sst.sel(time=time_slice).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(ds_zarr.where(ds_zarr.analysed_sst != -32768).analysed_sst[5318:5328,:,:].sel(lat=slice(40,50),lon=slice(-170,-160)).mean().values)\n",
    "ds_zarr.where(ds_zarr.analysed_sst != -32768).analysed_sst.sel(time=time_slice).mean().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After updating the S3 Zarr store with the new year, test that we can open it and it also matches the NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True, client_kwargs=dict(region_name='us-east-1'))\n",
    "s3_store = s3fs.S3Map(root='nasa-eodc/eodc/mursst_zarr/5x1799x3600', s3=s3, check=False)\n",
    "ds_zarr = xr.open_zarr(s3_store, consolidated=True, mask_and_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(ds_zarr.where(ds_zarr.analysed_sst != -32768).analysed_sst[4577:4597,:,:].sel(lat=slice(40,50),lon=slice(-170,-160)).mean().values)\n",
    "ds_zarr.where(ds_zarr.analysed_sst != -32768).analysed_sst.sel(time=time_slice).mean().values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:netcdf_to_zarr] *",
   "language": "python",
   "name": "conda-env-netcdf_to_zarr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
