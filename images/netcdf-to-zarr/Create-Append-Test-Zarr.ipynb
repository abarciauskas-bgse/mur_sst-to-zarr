{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zarr store directory: /fsx/eodc/mursst_zarr/1x1799x3600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numcodecs\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "import zarr\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def generate_file_list(start_doy, end_doy):   \n",
    "    \"\"\"\n",
    "    Given a start day and end end day, generate a list of file locations.\n",
    "    Assumes a 'prefix' and 'year' variables have already been defined.\n",
    "    'Prefix' should be a local directory or http url and path.\n",
    "    'Year' should be a 4 digit year.\n",
    "    \"\"\"\n",
    "    days_of_year = list(range(start_doy, end_doy))\n",
    "    fileObjs = []\n",
    "    for doy in days_of_year:\n",
    "        if doy < 10:\n",
    "            doy = f\"00{doy}\"\n",
    "        elif doy >= 10 and doy < 100:\n",
    "            doy = f\"0{doy}\"            \n",
    "        file = glob.glob(f\"{prefix}/{doy}/*.nc\")[0]\n",
    "        fileObjs.append(file)\n",
    "    return fileObjs\n",
    "\n",
    "# Invariants - but should be made configurable\n",
    "year = 2002\n",
    "prefix = f\"/fsx/eodc/mursst_netcdf/{year}\"\n",
    "chunks = {'time': 1, 'lat': 1799, 'lon': 3600}\n",
    "path = 'x'.join(map(str, chunks.values()))\n",
    "store_dir = f\"/fsx/eodc/mursst_zarr/{path}\"\n",
    "numcodecs.blosc.use_threads = False\n",
    "print(f\"zarr store directory: {store_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask client <Client: 'tcp://127.0.0.1:45917' processes=4 threads=32, memory=125.83 GB>\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCluster(n_workers=4)\n",
    "client = Client(cluster)\n",
    "print(f\"Dask client {client}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start doy: 357, file: 20021223090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\n",
      "end doy: 366, file: 20021231090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\n",
      "Done with this batch\n",
      "\n",
      "CPU times: user 22 s, sys: 2.07 s, total: 24 s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop and append\n",
    "start_doy = 357\n",
    "end_doy = start_doy\n",
    "number_batches_to_append = 1\n",
    "batch_size = 9\n",
    "final_end_doy = start_doy + (number_batches_to_append * batch_size)\n",
    "\n",
    "while start_doy < final_end_doy:\n",
    "    end_doy = start_doy + batch_size\n",
    "    end_doy = min(366, end_doy)\n",
    "    fileObjs = generate_file_list(start_doy, end_doy)\n",
    "    first_file = fileObjs[0].split('/')[-1]\n",
    "    last_file = fileObjs[-1].split('/')[-1]\n",
    "    print(f\"start doy: {start_doy}, file: {first_file}\")\n",
    "    print(f\"end doy: {end_doy}, file: {last_file}\")\n",
    "    # Check here that the next day we will append is the next day in the year\n",
    "    current_ds = xr.open_zarr(store_dir)\n",
    "    next_day = current_ds.time[-1].values + np.timedelta64(1,'D')\n",
    "    next_day_str = str(next_day)[0:10].replace('-', '') \n",
    "    if not (first_file[0:8] == next_day_str):\n",
    "        raise Exception(\"starting file is not the next day of the year\")\n",
    "        break\n",
    "    else:\n",
    "        # Open dataset\n",
    "        ds = xr.open_mfdataset(fileObjs, chunks=chunks, parallel=True, combine='by_coords')\n",
    "        ds = ds.astype(np.float64)\n",
    "        # Either append or initiate store\n",
    "        args = {'consolidated': True}\n",
    "        if start_doy == 152 and year == 2002:\n",
    "            args['mode'] = 'w'\n",
    "        else:\n",
    "            args['mode'] = 'a'\n",
    "            args['append_dim'] = 'time'\n",
    "        ds.to_zarr(store_dir, **args)\n",
    "    start_doy = end_doy\n",
    "    print(f\"Done with this batch\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the output\n",
    "\n",
    "Assuming we are using 1x1799x3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start doy: 152, file: 20020601090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\n",
      "end doy: 366, file: 20021231090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\n",
      "CPU times: user 3.34 s, sys: 333 ms, total: 3.68 s\n",
      "Wall time: 5.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time_slice = slice(datetime.strptime(f\"{year}-06-10\", '%Y-%m-%d'), datetime.strptime(f\"{year}-06-30\", '%Y-%m-%d'))\n",
    "\n",
    "start_doy = 152\n",
    "end_doy = 366\n",
    "\n",
    "fileObjs = generate_file_list(start_doy, end_doy)\n",
    "print(f\"start doy: {start_doy}, file: {fileObjs[0].split('/')[-1]}\")\n",
    "print(f\"end doy: {end_doy}, file: {fileObjs[-1].split('/')[-1]}\")          \n",
    "ds_netcdf = xr.open_mfdataset(fileObjs, chunks=chunks, parallel=True, combine='by_coords')\n",
    "ds_netcdf = ds_netcdf.astype(np.float64)\n",
    "ds_zarr = xr.open_zarr('/fsx/eodc/mursst_zarr/1x1799x3600')\n",
    "assert(ds_netcdf.dims == ds_zarr.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294.760208857499\n",
      "CPU times: user 2.13 s, sys: 143 ms, total: 2.27 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(ds_netcdf.analysed_sst[150:214,:,:].sel(lat=slice(0,50),lon=slice(-170,-110)).mean().values)\n",
    "#ds_netcdf.analysed_sst.sel(time=time_slice).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294.760208857499\n",
      "CPU times: user 1.44 s, sys: 85.1 ms, total: 1.53 s\n",
      "Wall time: 3.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(ds_zarr.analysed_sst[150:214,:,:].sel(lat=slice(0,50),lon=slice(-170,-110)).mean().values)\n",
    "#ds_zarr.analysed_sst.sel(time=time_slice).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False, client_kwargs=dict(region_name='us-east-1'))\n",
    "s3_dir = 'ds-data-projects/eodc/eodc/mursst_zarr/1x1799x3600'\n",
    "s3_store = s3fs.S3Map(root=s3_dir, s3=s3, check=True)\n",
    "ds=xr.open_zarr(s3_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:           (lat: 17999, lon: 36000, time: 214)\n",
       "Coordinates:\n",
       "  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n",
       "  * lon               (lon) float32 -179.99 -179.98 -179.97 ... 179.99 180.0\n",
       "  * time              (time) datetime64[ns] 2002-06-01T09:00:00 ... 2002-12-31T09:00:00\n",
       "Data variables:\n",
       "    analysed_sst      (time, lat, lon) float64 dask.array&lt;chunksize=(1, 1799, 3600), meta=np.ndarray&gt;\n",
       "    analysis_error    (time, lat, lon) float64 dask.array&lt;chunksize=(1, 1799, 3600), meta=np.ndarray&gt;\n",
       "    mask              (time, lat, lon) float64 dask.array&lt;chunksize=(1, 1799, 3600), meta=np.ndarray&gt;\n",
       "    sea_ice_fraction  (time, lat, lon) float64 dask.array&lt;chunksize=(1, 1799, 3600), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:           (lat: 17999, lon: 36000, time: 214)\n",
       "Coordinates:\n",
       "  * lat               (lat) float32 -89.99 -89.98 -89.97 ... 89.97 89.98 89.99\n",
       "  * lon               (lon) float32 -179.99 -179.98 -179.97 ... 179.99 180.0\n",
       "  * time              (time) datetime64[ns] 2002-06-01T09:00:00 ... 2002-12-31T09:00:00\n",
       "Data variables:\n",
       "    analysed_sst      (time, lat, lon) float64 dask.array<chunksize=(1, 1799, 3600), meta=np.ndarray>\n",
       "    analysis_error    (time, lat, lon) float64 dask.array<chunksize=(1, 1799, 3600), meta=np.ndarray>\n",
       "    mask              (time, lat, lon) float64 dask.array<chunksize=(1, 1799, 3600), meta=np.ndarray>\n",
       "    sea_ice_fraction  (time, lat, lon) float64 dask.array<chunksize=(1, 1799, 3600), meta=np.ndarray>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance testing different chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_opt 1x1799x3600\n",
      "chunk_opt 5x1000x1000\n",
      "chunk_opt 1x3600x7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1x1799x3600': 1.7027885675430299,\n",
       " '5x1000x1000': 2.600662112236023,\n",
       " '1x3600x7200': 1.9275601387023926}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_slice = slice(datetime.strptime(f\"{year}-06-02\", '%Y-%m-%d'), datetime.strptime(f\"{year}-06-04\", '%Y-%m-%d'))\n",
    "chunk_opts = ['1x1799x3600', '5x1000x1000', '1x3600x7200']\n",
    "n_tests = 10\n",
    "test_results = {}\n",
    "for chunk_opt in chunk_opts:\n",
    "    print(f\"chunk_opt {chunk_opt}\")\n",
    "    ds_zarr = xr.open_zarr(f\"/fsx/eodc/mursst_zarr/{chunk_opt}\")\n",
    "    durations = []\n",
    "    for testi in range(n_tests):\n",
    "        start = time.time()\n",
    "        ds_zarr.analysed_sst[1,:,:].sel(lat=slice(0,50),lon=slice(-170,-110)).mean().values\n",
    "        ds_zarr.analysed_sst.sel(time=time_slice).mean().values\n",
    "        end = time.time()\n",
    "        durations.append(end - start)\n",
    "    test_results[chunk_opt] = np.sum(durations)/n_tests\n",
    "        \n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_opt 1x1799x3600\n",
      "chunk_opt 5x1000x1000\n",
      "chunk_opt 1x3600x7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1x1799x3600': 1.63055739402771,\n",
       " '5x1000x1000': 2.6444275856018065,\n",
       " '1x3600x7200': 1.914821243286133}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_slice = slice(datetime.strptime(f\"{year}-06-02\", '%Y-%m-%d'), datetime.strptime(f\"{year}-06-04\", '%Y-%m-%d'))\n",
    "chunk_opts = ['1x1799x3600', '5x1000x1000', '1x3600x7200']\n",
    "n_tests = 10\n",
    "test_results = {}\n",
    "for chunk_opt in chunk_opts:\n",
    "    print(f\"chunk_opt {chunk_opt}\")\n",
    "    ds_zarr = xr.open_zarr(f\"/fsx/eodc/mursst_zarr/{chunk_opt}\")\n",
    "    durations = []\n",
    "    for testi in range(n_tests):\n",
    "        start = time.time()\n",
    "        ds_zarr.analysed_sst[1,:,:].sel(lat=slice(0,50),lon=slice(-170,-110)).mean().values\n",
    "        ds_zarr.analysed_sst.sel(time=time_slice).mean().values\n",
    "        end = time.time()\n",
    "        durations.append(end - start)\n",
    "    test_results[chunk_opt] = np.sum(durations)/n_tests\n",
    "        \n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best option is 1x1799x3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Failures through by creating a subset and replacing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# start_doy = 157\n",
    "# end_doy = 162 \n",
    "# fileObjs = generate_file_list(start_doy, end_doy)\n",
    "# print(f\"start doy: {start_doy}, file: {fileObjs[0].split('/')[-1]}\")\n",
    "# print(f\"end doy: {end_doy}, file: {fileObjs[-1].split('/')[-1]}\")          \n",
    "# ds = xr.open_mfdataset(fileObjs, chunks=chunks, parallel=True, combine='by_coords')\n",
    "# ds = ds.astype(np.float64)\n",
    "# subset_dir = 'subset'\n",
    "# ds.to_zarr(subset_dir, consolidated=True, mode='w')\n",
    "# existing_group = zarr.open(store=store_dir)\n",
    "# subset_group = zarr.open(store=subset_dir)\n",
    "# zarr.copy(subset_group, existing_group, name='mursst', if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:netcdf_to_zarr] *",
   "language": "python",
   "name": "conda-env-netcdf_to_zarr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
